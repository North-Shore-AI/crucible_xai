# CrucibleXAI v0.3.0+ Implementation Roadmap

**Date**: October 29, 2025
**Current Version**: v0.2.1 (277 tests, 94.1% coverage, 17 methods)
**Author**: North Shore AI
**Status**: Active Planning Document

---

## Executive Summary

CrucibleXAI v0.2.1 successfully delivers a comprehensive XAI library with 17 attribution methods, covering local explanations (LIME, SHAP variants, gradient methods, occlusion), global interpretability (PDP, ICE, ALE, H-statistic), and parallel batch processing. This document outlines the technical roadmap for v0.3.0 and beyond, prioritizing TreeSHAP, validation metrics, and counterfactual explanations.

---

## Current State (v0.2.1)

### Completed Features

**Local Attribution Methods (10)**:
- ✅ LIME (all features complete)
- ✅ KernelSHAP (model-agnostic approximation)
- ✅ LinearSHAP (exact for linear models, <2ms)
- ✅ SamplingShap (Monte Carlo approximation)
- ✅ Gradient × Input (Nx autodiff)
- ✅ Integrated Gradients (completeness axiom)
- ✅ SmoothGrad (noise reduction)
- ✅ Feature Occlusion (model-agnostic)
- ✅ Sliding Window Occlusion (sequential data)
- ✅ Occlusion Sensitivity (normalized)

**Global Interpretability Methods (7)**:
- ✅ Permutation Importance
- ✅ PDP 1D (marginal effects)
- ✅ PDP 2D (interactions)
- ✅ ICE (per-instance curves)
- ✅ Centered ICE
- ✅ ALE (robust for correlations)
- ✅ H-Statistic (interaction detection)

**Infrastructure**:
- ✅ Parallel batch processing (LIME, SHAP, Occlusion)
- ✅ Configurable concurrency and error handling
- ✅ 277 tests with 100% pass rate
- ✅ >94% code coverage

### Gaps & Future Work

**Missing SHAP Variant**:
- ❌ TreeSHAP (for tree-based models)

**No Validation Tools**:
- ❌ Faithfulness metrics
- ❌ Sensitivity analysis
- ❌ Robustness testing

**No Counterfactuals**:
- ❌ DiCE or similar
- ❌ Actionable "what if" explanations

**Limited Neural Network Support**:
- ❌ LRP, DeepLIFT, GradCAM
- ❌ Attention visualization

---

## OPTION 1: TreeSHAP Implementation (v0.3.0)

### Overview

TreeSHAP provides **exact** SHAP values for tree-based models (decision trees, random forests, gradient boosting) in **polynomial time** instead of exponential. This is a high-value addition given the popularity of tree models.

### Technical Specification

#### Core Algorithm

```elixir
defmodule CrucibleXAI.SHAP.TreeSHAP do
  @moduledoc """
  TreeSHAP: Efficient exact SHAP computation for tree-based models.

  Unlike KernelSHAP's approximation, TreeSHAP computes exact Shapley
  values by traversing the tree structure. Complexity is O(TLD²) where:
  - T = number of trees
  - L = max tree depth
  - D = max number of leaves

  For typical models: ~1ms vs ~1s for KernelSHAP
  """

  @doc """
  Compute exact SHAP values for a tree model.

  ## Parameters
    * `instance` - Instance to explain
    * `tree_model` - Tree model structure (see TreeModel protocol)
    * `background_data` - Background for baseline (optional)

  ## Returns
    Map of feature_index => exact_shapley_value
  """
  @spec explain(list(), TreeModel.t(), list(), keyword()) :: %{integer() => float()}
  def explain(instance, tree_model, background_data, opts \\ [])

  @doc """
  Traverse tree and compute path-dependent SHAP values.

  ## Algorithm
  1. For each tree in the ensemble:
     - Recursively traverse from root
     - Track feature subsets at each split
     - Compute contribution at leaves
  2. Aggregate across all trees
  3. Return exact Shapley values
  """
  @spec tree_path_dependent_shap(TreeModel.t(), list()) :: %{integer() => float()}
  def tree_path_dependent_shap(tree, instance)
end
```

#### Tree Model Protocol

```elixir
defprotocol CrucibleXAI.TreeModel do
  @moduledoc """
  Protocol for tree-based models to support TreeSHAP.

  Implementations needed for:
  - Decision trees
  - Random forests
  - Gradient boosted trees (XGBoost-style)
  - Axon tree models (if applicable)
  """

  @doc "Get root node of the tree"
  def get_root(model)

  @doc "Get children of a node"
  def get_children(model, node)

  @doc "Get split feature and threshold for internal node"
  def get_split_info(model, node)

  @doc "Get prediction value for leaf node"
  def get_leaf_value(model, node)

  @doc "Check if node is leaf"
  def is_leaf?(model, node)
end
```

#### Data Structures

```elixir
defmodule CrucibleXAI.TreeModel.Node do
  @moduledoc "Represents a node in a decision tree"

  defstruct [
    :node_id,
    :feature_index,    # nil for leaf
    :threshold,        # nil for leaf
    :left_child,       # nil for leaf
    :right_child,      # nil for leaf
    :value,            # prediction value (for leaf)
    :cover,            # number of samples (optional)
    :is_leaf
  ]
end

defmodule CrucibleXAI.TreeModel.Tree do
  @moduledoc "Represents a decision tree"

  defstruct [
    :nodes,           # Map of node_id => Node
    :root_id,         # ID of root node
    :num_features,    # Total number of features
    :base_value       # Base prediction (optional)
  ]
end
```

### Implementation Plan (TDD)

#### Week 1: Tree Structure & Protocol

**RED Phase**:
```elixir
# test/crucible_xai/tree_model_test.exs
test "creates simple decision tree" do
  tree = TreeModel.Tree.new([
    %Node{node_id: 0, feature_index: 0, threshold: 5.0, left_child: 1, right_child: 2},
    %Node{node_id: 1, value: 10.0, is_leaf: true},
    %Node{node_id: 2, value: 20.0, is_leaf: true}
  ])

  assert TreeModel.get_root(tree).node_id == 0
  assert TreeModel.is_leaf?(tree, TreeModel.get_root(tree)) == false
end

test "traverses tree for prediction" do
  # Tree: if x[0] < 5 then 10 else 20
  tree = create_simple_tree()

  assert TreeModel.predict(tree, [3.0]) == 10.0
  assert TreeModel.predict(tree, [7.0]) == 20.0
end
```

**GREEN Phase**:
- Implement TreeModel protocol
- Implement Node and Tree structs
- Basic tree traversal
- Prediction function

**Estimated Effort**: 3-4 days

#### Week 2: Core TreeSHAP Algorithm

**RED Phase**:
```elixir
# test/crucible_xai/shap/tree_shap_test.exs
test "computes exact SHAP for simple tree" do
  # Tree: if x[0] < 5 then 10 else 20
  tree = create_simple_tree()
  instance = [7.0]

  shap_values = TreeSHAP.explain(instance, tree, [[0.0], [10.0]])

  # For instance [7.0], goes right (value=20)
  # Baseline (mean of background): (10+20)/2 = 15
  # SHAP should attribute difference: 20 - 15 = 5 to feature 0
  assert_in_delta shap_values[0], 5.0, 0.01
end

test "handles multi-level tree" do
  # Tree with depth 3
  tree = create_multi_level_tree()
  instance = [3.0, 7.0]

  shap_values = TreeSHAP.explain(instance, tree, background)

  # Should have exact values for both features
  assert map_size(shap_values) == 2
  # Verify additivity
  assert_additivity(shap_values, instance, tree, background)
end

test "matches KernelSHAP results" do
  tree = create_simple_tree()
  instance = [5.0, 3.0]
  background = generate_background()

  tree_shap = TreeSHAP.explain(instance, tree, background)
  kernel_shap = KernelSHAP.explain(instance, background, &TreeModel.predict(tree, &1))

  # TreeSHAP should be exact, KernelSHAP approximate
  # Should be close but TreeSHAP is ground truth
  assert_maps_close(tree_shap, kernel_shap, tolerance: 0.5)
end
```

**GREEN Phase**:
- Implement recursive tree traversal with SHAP computation
- Track feature subsets at each node
- Compute leaf contributions
- Aggregate across paths

**Estimated Effort**: 5-7 days

#### Week 3: Ensemble Support & Optimization

**Features**:
- Support for random forests (average SHAP across trees)
- Support for gradient boosting (sum SHAP across trees)
- Parallel tree processing
- Caching for repeated instances

**Estimated Effort**: 3-5 days

#### Week 4: Testing & Documentation

**Tasks**:
- Property-based tests (additivity, symmetry, dummy)
- Performance benchmarks vs KernelSHAP
- Integration with main SHAP API
- Examples and documentation
- Comparison guide (when to use TreeSHAP vs others)

**Estimated Effort**: 3-4 days

### Expected Outcomes

**Performance**:
- TreeSHAP: **~1ms** per explanation (1000x faster than KernelSHAP)
- Exact values (no approximation error)
- Scales to large ensembles

**Test Coverage**:
- 25-30 new tests
- Property-based testing for tree properties
- Integration tests with existing SHAP

**Total Time**: ~3 weeks with TDD

---

## OPTION 2: Validation & Quality Metrics Suite (v0.3.0)

### Overview

Build tools to **validate and measure quality** of explanations. Critical for production deployments where trust and reliability are paramount.

### Technical Specification

#### 2.1 Faithfulness Metrics

**Objective**: Measure how well explanations reflect actual model behavior.

```elixir
defmodule CrucibleXAI.Validation.Faithfulness do
  @moduledoc """
  Faithfulness metrics for explanation quality.

  Measures how well explanations capture model behavior.
  """

  @doc """
  Faithfulness via feature removal.

  Remove top-k features according to explanation and measure
  prediction change. Good explanations should cause large changes.

  ## Algorithm
  1. Get explanation for instance
  2. Sort features by attribution (descending)
  3. For k = 1 to N:
     - Remove top k features (set to baseline)
     - Measure prediction drop
  4. Return correlation between attribution and prediction drop

  ## Parameters
    * `instance` - Instance to test
    * `explanation` - Explanation to validate
    * `predict_fn` - Prediction function
    * `opts` - Options:
      * `:baseline_value` - Value for removed features (default: 0.0)
      * `:num_steps` - Number of features to test (default: all)

  ## Returns
    Map with:
    * `:correlation` - Correlation between attribution rank and importance
    * `:prediction_drops` - List of prediction changes
    * `:faithfulness_score` - Overall score (0-1, higher is better)
  """
  @spec measure_faithfulness(list(), Explanation.t(), function(), keyword()) :: map()
  def measure_faithfulness(instance, explanation, predict_fn, opts \\ [])

  @doc """
  Monotonicity test.

  Predictions should decrease monotonically as we remove features
  in order of their attribution.

  ## Returns
    Boolean indicating if monotonicity holds
  """
  @spec monotonicity_test(list(), Explanation.t(), function()) :: boolean()
  def monotonicity_test(instance, explanation, predict_fn)
end
```

#### 2.2 Infidelity Metric

```elixir
defmodule CrucibleXAI.Validation.Infidelity do
  @moduledoc """
  Infidelity: measures explanation error.

  Lower is better. Compares explanation's predicted change
  vs actual model change when features are perturbed.
  """

  @doc """
  Compute infidelity score.

  ## Formula
  Infidelity = E[(f(x) - f(x̃) - φ·(x - x̃))²]

  where:
  - f is the model
  - x̃ is perturbed input
  - φ is the attribution vector

  ## Algorithm
  1. Generate perturbations of the instance
  2. For each perturbation:
     - Compute actual change: f(x) - f(x̃)
     - Compute predicted change: φ·(x - x̃)
     - Measure squared error
  3. Average squared errors

  ## Returns
    Float (lower is better, 0 = perfect)
  """
  @spec compute(list(), map(), function(), keyword()) :: float()
  def compute(instance, attribution_map, predict_fn, opts \\ [])
end
```

#### 2.3 Sensitivity Analysis

```elixir
defmodule CrucibleXAI.Validation.Sensitivity do
  @moduledoc """
  Sensitivity tests for explanation robustness.
  """

  @doc """
  Test sensitivity to input perturbations.

  Good explanations should be stable under small input changes.

  ## Algorithm
  1. Generate small perturbations of instance
  2. Compute explanations for each perturbation
  3. Measure variance in attributions
  4. Return stability score

  ## Returns
    Map with:
    * `:mean_variation` - Average change in attributions
    * `:max_variation` - Maximum change observed
    * `:stability_score` - 1 - (variation / baseline), higher is better
  """
  @spec input_sensitivity(list(), function(), function(), keyword()) :: map()
  def input_sensitivity(instance, explain_fn, predict_fn, opts \\ [])

  @doc """
  Test sensitivity to explanation parameters.

  Measures how sensitive explanations are to hyperparameters
  (e.g., num_samples in LIME).
  """
  @spec parameter_sensitivity(list(), function(), function(), keyword()) :: map()
  def parameter_sensitivity(instance, explain_fn, predict_fn, opts \\ [])
end
```

#### 2.4 Completeness & Consistency

```elixir
defmodule CrucibleXAI.Validation.Axioms do
  @moduledoc """
  Test explanations against theoretical axioms.
  """

  @doc """
  Test completeness (for methods that claim it).

  For Integrated Gradients: sum(attributions) ≈ f(x) - f(baseline)
  For SHAP: sum(SHAP values) ≈ f(x) - E[f(x)]
  """
  @spec test_completeness(map(), list(), function(), keyword()) :: boolean()
  def test_completeness(attributions, instance, predict_fn, opts \\ [])

  @doc """
  Test symmetry axiom.

  Features with identical marginal contributions should have
  identical SHAP values.
  """
  @spec test_symmetry(map(), list(), function()) :: boolean()
  def test_symmetry(shap_values, instance, predict_fn)

  @doc """
  Test dummy axiom.

  Features that don't affect the output should have zero attribution.
  """
  @spec test_dummy(map(), list(), function()) :: boolean()
  def test_dummy(attributions, instance, predict_fn)
end
```

### Implementation Plan (TDD)

#### Week 1: Faithfulness Metrics

**RED Phase** (2 days):
- 8-10 tests for feature removal faithfulness
- 5-7 tests for monotonicity
- Property-based tests for correlation

**GREEN Phase** (2 days):
- Implement feature removal algorithm
- Compute prediction drops
- Calculate correlations
- Return structured results

**DEBUG Phase** (1 day):
- Handle edge cases
- Optimize for batch processing

#### Week 2: Infidelity & Sensitivity

**RED Phase** (2 days):
- 6-8 tests for infidelity computation
- 8-10 tests for sensitivity analysis
- Tests for different perturbation strategies

**GREEN Phase** (2-3 days):
- Implement perturbation generation
- Compute infidelity scores
- Sensitivity variance calculation

**DEBUG Phase** (1 day):
- Numerical stability
- Performance optimization

#### Week 3: Axioms & Integration

**RED Phase** (1 day):
- 5-7 tests for axiom checking
- Integration tests with existing methods

**GREEN Phase** (2 days):
- Implement completeness checks
- Symmetry and dummy tests
- Integration with Explanation struct

**DEBUG Phase** (1 day):
- Documentation
- Examples
- Validation suite CLI

### Expected Outcomes

**Deliverables**:
- Comprehensive validation suite
- 25-30 new tests
- Quality scoring for explanations
- Production readiness tools

**Use Cases**:
- A/B testing different explanation methods
- Monitoring explanation quality in production
- Debugging poor explanations
- Model certification

**Total Time**: ~3 weeks with TDD

---

## OPTION 3: Counterfactual Explanations (v0.4.0)

### Overview

Counterfactuals answer: **"What changes would flip the prediction?"**

Example: "If your income was $5k higher and debt ratio was 5% lower, you would be approved for the loan."

### Technical Specification

#### 3.1 DiCE (Diverse Counterfactual Explanations)

```elixir
defmodule CrucibleXAI.Counterfactual.DiCE do
  @moduledoc """
  DiCE: Diverse Counterfactual Explanations

  Generates multiple diverse counterfactual examples that achieve
  a desired prediction outcome.

  ## Key Features
  - Diversity: Generate varied counterfactuals
  - Proximity: Close to original instance
  - Actionability: Only change mutable features
  - Plausibility: Stay within data distribution
  """

  @doc """
  Generate diverse counterfactuals.

  ## Parameters
    * `instance` - Original instance
    * `predict_fn` - Prediction function
    * `desired_outcome` - Target prediction or class
    * `opts` - Options:
      * `:num_counterfactuals` - Number to generate (default: 5)
      * `:diversity_weight` - Balance proximity vs diversity (default: 1.0)
      * `:mutable_features` - List of features that can change (default: all)
      * `:feature_ranges` - Valid ranges for each feature
      * `:categorical_features` - Indices of categorical features
      * `:optimization_method` - :gradient, :genetic, or :random (default: :genetic)
      * `:max_iterations` - Maximum optimization steps (default: 1000)

  ## Returns
    List of counterfactual instances with metadata:
    * `:instance` - The counterfactual instance
    * `:prediction` - Prediction for counterfactual
    * `:distance` - Distance from original
    * `:changes` - Map of feature => (old_value, new_value)
    * `:validity` - Whether it achieves desired outcome
  """
  @spec generate(list(), function(), any(), keyword()) :: list(map())
  def generate(instance, predict_fn, desired_outcome, opts \\ [])

  @doc """
  Optimize for diversity among counterfactuals.

  Uses diversity loss: L = proximity_loss + λ * diversity_loss

  ## Diversity Metrics
  - Determinantal Point Process (DPP)
  - Average pairwise distance
  - Feature-wise diversity
  """
  @spec optimize_diversity(list(list()), float()) :: list(list())
  def optimize_diversity(candidates, diversity_weight)
end
```

#### 3.2 Constraint Handling

```elixir
defmodule CrucibleXAI.Counterfactual.Constraints do
  @moduledoc """
  Constraint handling for counterfactual generation.
  """

  @doc """
  Actionability constraint.

  Only certain features can be changed (e.g., can't change age, race).
  """
  @spec actionable?(list(), list(), list(integer())) :: boolean()
  def actionable?(original, counterfactual, mutable_features)

  @doc """
  Plausibility constraint.

  Counterfactual should be within the data distribution.
  Uses kernel density estimation or range checks.
  """
  @spec plausible?(list(), list(list()), keyword()) :: boolean()
  def plausible?(counterfactual, training_data, opts \\ [])

  @doc """
  Proximity constraint.

  Counterfactual should be close to original (minimal changes).
  """
  @spec compute_distance(list(), list(), atom()) :: float()
  def compute_distance(original, counterfactual, distance_metric \\ :euclidean)

  @doc """
  Categorical feature constraints.

  Categorical features must take valid discrete values.
  """
  @spec validate_categorical(list(), list(integer()), map()) :: boolean()
  def validate_categorical(counterfactual, categorical_indices, valid_values_map)
end
```

#### 3.3 Optimization Methods

```elixir
defmodule CrucibleXAI.Counterfactual.Optimizer do
  @moduledoc """
  Optimization strategies for counterfactual generation.
  """

  @doc """
  Genetic algorithm optimization.

  ## Algorithm
  1. Initialize population of candidate counterfactuals
  2. For each generation:
     - Evaluate fitness (prediction + proximity + diversity)
     - Select best candidates
     - Crossover and mutation
     - Apply constraints
  3. Return top-k diverse counterfactuals
  """
  @spec genetic_algorithm(list(), function(), any(), keyword()) :: list(list())
  def genetic_algorithm(instance, predict_fn, desired_outcome, opts \\ [])

  @doc """
  Gradient-based optimization (for differentiable models).

  Minimizes: L = |f(x') - target|² + λ₁||x' - x||² + λ₂·diversity
  """
  @spec gradient_descent(list(), function(), any(), keyword()) :: list(list())
  def gradient_descent(instance, predict_fn, desired_outcome, opts \\ [])

  @doc """
  Random search with constraints.

  Simple but effective baseline method.
  """
  @spec random_search(list(), function(), any(), keyword()) :: list(list())
  def random_search(instance, predict_fn, desired_outcome, opts \\ [])
end
```

### Implementation Plan (TDD)

#### Week 1: Basic Counterfactual Generation

**RED Phase** (2 days):
- 10-12 tests for simple counterfactual generation
- Tests for binary classification
- Tests for regression
- Constraint violation tests

**GREEN Phase** (3 days):
- Implement random search optimization
- Basic constraint checking
- Single counterfactual generation

**DEBUG Phase** (1 day):
- Edge cases
- Performance tuning

#### Week 2: DiCE with Diversity

**RED Phase** (2 days):
- 8-10 tests for diversity optimization
- Tests for multiple counterfactuals
- Tests for diversity metrics

**GREEN Phase** (3 days):
- Implement genetic algorithm
- Diversity loss functions
- Multi-counterfactual generation

**DEBUG Phase** (1 day):
- Convergence issues
- Diversity tuning

#### Week 3: Constraints & Advanced Features

**RED Phase** (2 days):
- 10-12 tests for all constraint types
- Actionability tests
- Plausibility tests
- Categorical feature tests

**GREEN Phase** (2-3 days):
- Implement all constraint types
- KDE for plausibility
- Categorical handling

**DEBUG Phase** (1-2 days):
- Integration with existing methods
- Performance optimization

#### Week 4: Gradient Optimization & Polish

**RED Phase** (1 day):
- 5-7 tests for gradient-based optimization

**GREEN Phase** (2 days):
- Implement gradient descent for Nx models
- Integration with gradient attribution

**DEBUG Phase** (1-2 days):
- Documentation
- Examples
- Visualization of counterfactuals

### Expected Outcomes

**Deliverables**:
- Complete counterfactual generation system
- 35-40 new tests
- Multiple optimization strategies
- Comprehensive constraint handling

**Use Cases**:
- Actionable recourse ("how to get approved")
- Model debugging (finding decision boundaries)
- Fairness analysis (comparing counterfactuals across groups)
- What-if scenario analysis

**Total Time**: ~4 weeks with TDD

---

## Additional Future Features (v0.5.0+)

### 4. Anchors (Rule-based Explanations)

**Objective**: High-precision IF-THEN rules

```elixir
defmodule CrucibleXAI.Anchors do
  @doc """
  Find anchor: minimal rule that "anchors" prediction.

  Example: "IF income > 50k AND age > 25 THEN approved (precision: 0.95)"
  """
  @spec explain(list(), function(), keyword()) :: map()
  def explain(instance, predict_fn, opts \\ [])
end
```

**Implementation**:
- Multi-armed bandit for rule search
- Beam search optimization
- Coverage and precision calculation

**Effort**: 2-3 weeks TDD

---

### 5. Layer-wise Relevance Propagation (LRP)

**Objective**: Attribution for neural networks via backward propagation

```elixir
defmodule CrucibleXAI.Neural.LRP do
  @doc """
  LRP with multiple propagation rules.

  Rules:
  - ε-rule: basic propagation with numerical stability
  - γ-rule: emphasizes positive contributions
  - α-β rule: separates positive and negative contributions
  """
  @spec calculate(AxonModel.t(), Nx.Tensor.t(), keyword()) :: Nx.Tensor.t()
  def calculate(model, instance, opts \\ [])
end
```

**Requires**:
- Axon model introspection
- Layer activation extraction
- Backward propagation logic

**Effort**: 3-4 weeks TDD

---

### 6. DeepLIFT & GradCAM

**DeepLIFT**:
```elixir
defmodule CrucibleXAI.Neural.DeepLIFT do
  @doc """
  DeepLIFT: Deep Learning Important FeaTures

  Compares neuron activations to reference baseline.
  """
  @spec calculate(AxonModel.t(), Nx.Tensor.t(), Nx.Tensor.t()) :: Nx.Tensor.t()
  def calculate(model, instance, baseline)
end
```

**GradCAM** (for CNNs):
```elixir
defmodule CrucibleXAI.Neural.GradCAM do
  @doc """
  GradCAM: Gradient-weighted Class Activation Mapping

  Creates visual saliency maps for CNN predictions.
  """
  @spec calculate(AxonModel.t(), Nx.Tensor.t(), integer(), integer()) :: Nx.Tensor.t()
  def calculate(model, image, target_layer, target_class)
end
```

**Effort**: 4-5 weeks TDD (both combined)

---

### 7. Performance & Production Features

#### 7.1 Caching System

```elixir
defmodule CrucibleXAI.Cache do
  @doc """
  Cache explanations for repeated instances.

  Uses ETS for in-memory caching with TTL.
  """
  @spec get_or_compute(binary(), function()) :: any()
  def get_or_compute(cache_key, compute_fn)

  @doc """
  Cache LIME samples for repeated use.
  """
  @spec cache_samples(list(), list(list()), keyword()) :: :ok
  def cache_samples(instance, samples, opts \\ [])
end
```

**Effort**: 1 week TDD

#### 7.2 GPU Acceleration

```elixir
# Enable EXLA backend for GPU
Nx.default_backend(EXLA.Backend)

# Batch operations automatically use GPU
explanations = CrucibleXai.explain_batch(
  instances,
  predict_fn,
  parallel: true,
  backend: EXLA.Backend
)
```

**Tasks**:
- EXLA backend integration
- Benchmark GPU vs CPU
- Optimize tensor operations for GPU

**Effort**: 1-2 weeks

#### 7.3 Streaming for Large Datasets

```elixir
defmodule CrucibleXAI.Streaming do
  @doc """
  Stream explanations for large datasets.

  Processes data in chunks to manage memory.
  """
  @spec stream_explanations(Stream.t(), function(), keyword()) :: Stream.t()
  def stream_explanations(data_stream, predict_fn, opts \\ [])
end
```

**Effort**: 1 week TDD

---

### 8. Persistence & Comparison

```elixir
defmodule CrucibleXAI.Persistence do
  @doc """
  Save explanations to disk.
  """
  @spec save(Explanation.t() | map(), Path.t()) :: :ok | {:error, term()}
  def save(explanation, path)

  @doc """
  Load explanations from disk.
  """
  @spec load(Path.t()) :: {:ok, Explanation.t()} | {:error, term()}
  def load(path)
end

defmodule CrucibleXAI.Comparison do
  @doc """
  Compare explanations across model versions.
  """
  @spec compare_models(list(), list(), function(), function()) :: map()
  def compare_models(instances, model_v1_fn, model_v2_fn)

  @doc """
  Measure explanation similarity.
  """
  @spec similarity(Explanation.t(), Explanation.t()) :: float()
  def similarity(exp1, exp2)
end
```

**Effort**: 1-2 weeks TDD

---

### 9. Crucible Ecosystem Integration

```elixir
defmodule CrucibleXAI.Integrations.Crucible do
  @doc """
  Seamless integration with Crucible models.
  """
  @spec explain_crucible_model(CrucibleModel.t(), list(), keyword()) :: Explanation.t()
  def explain_crucible_model(model, instance, opts \\ [])
end

defmodule CrucibleXAI.Integrations.CrucibleBench do
  @doc """
  Statistical testing of explanation differences.
  """
  @spec test_significance(list(Explanation.t()), list(Explanation.t())) :: map()
  def test_significance(explanations_a, explanations_b)
end
```

**Effort**: 1-2 weeks

---

### 10. Domain-Specific Extensions

#### NLP

```elixir
defmodule CrucibleXAI.Domain.NLP do
  @doc "Token-level importance for text models"
  @spec token_importance(function(), String.t(), module()) :: map()
  def token_importance(model, text, tokenizer)

  @doc "Attention visualization for transformers"
  @spec visualize_attention(AxonModel.t(), String.t()) :: map()
  def visualize_attention(model, text)
end
```

**Effort**: 2-3 weeks TDD

#### Computer Vision

```elixir
defmodule CrucibleXAI.Domain.Vision do
  @doc "Saliency maps for image classification"
  @spec saliency_map(AxonModel.t(), Nx.Tensor.t()) :: Nx.Tensor.t()
  def saliency_map(model, image)

  @doc "Segmentation-based explanations"
  @spec segment_attribution(AxonModel.t(), Nx.Tensor.t(), keyword()) :: map()
  def segment_attribution(model, image, opts \\ [])
end
```

**Effort**: 2-3 weeks TDD

#### Time Series

```elixir
defmodule CrucibleXAI.Domain.TimeSeries do
  @doc "Temporal LIME for time series"
  @spec temporal_lime(list(), function(), keyword()) :: map()
  def temporal_lime(time_series, predict_fn, opts \\ [])

  @doc "Event attribution in sequences"
  @spec event_attribution(list(), function(), integer()) :: map()
  def event_attribution(time_series, predict_fn, event_idx)
end
```

**Effort**: 2-3 weeks TDD

---

## Implementation Priority Matrix

| Feature | Impact | Effort | Priority | Dependencies |
|---------|--------|--------|----------|--------------|
| **TreeSHAP** | High | Medium | **P0** | None |
| **Validation Suite** | High | Medium | **P0** | None |
| **Counterfactuals** | High | High | **P1** | None |
| **Caching** | Medium | Low | **P1** | None |
| **Anchors** | Medium | Medium | P2 | None |
| **LRP** | Medium | High | P2 | Axon integration |
| **DeepLIFT** | Medium | High | P2 | Axon integration |
| **GradCAM** | Medium | Medium | P2 | Axon integration |
| **Persistence** | Low | Low | P3 | None |
| **GPU Acceleration** | Medium | Medium | P3 | EXLA |
| **Domain-Specific** | Low | High | P3 | Core methods |

**Priority Levels**:
- **P0**: Critical for v0.3.0 (next release)
- **P1**: Important for v0.4.0
- **P2**: Nice-to-have for v0.5.0
- **P3**: Future enhancements

---

## Estimated Timeline

### v0.3.0 (Next Release)
**Target**: 6-8 weeks from now

Core Features:
- TreeSHAP (3 weeks TDD)
- Validation Suite (3 weeks TDD)
- Caching (1 week TDD)
- Performance optimization (1 week)

**Total**: ~300-320 tests

---

### v0.4.0
**Target**: Q1 2026

Core Features:
- Counterfactuals/DiCE (4 weeks TDD)
- Anchors (3 weeks TDD)
- Persistence & Comparison (2 weeks)

**Total**: ~360-380 tests

---

### v0.5.0
**Target**: Q2 2026

Core Features:
- LRP (3 weeks TDD)
- DeepLIFT (2 weeks TDD)
- GradCAM (2 weeks TDD)
- Axon integration (2 weeks)

**Total**: ~420-450 tests

---

### v0.6.0+
**Target**: Q3 2026 onwards

- Domain-specific tools
- Advanced visualizations
- Crucible ecosystem integration
- Research features

---

## Success Metrics

### Technical Metrics
- **Test Coverage**: >95%
- **Performance**: All methods <1s except batch operations
- **Reliability**: 100% test pass rate
- **Documentation**: 100% API coverage

### Adoption Metrics
- 1000+ Hex downloads
- 100+ GitHub stars
- 10+ production deployments
- Active community contributions

### Research Impact
- Conference publications
- Benchmark comparisons with Python libraries
- Novel techniques in Elixir/Nx ecosystem

---

## Conclusion

CrucibleXAI v0.2.1 has established a **world-class foundation** with 17 methods and 277 tests. The roadmap forward focuses on:

1. **Completing SHAP family** with TreeSHAP
2. **Production readiness** with validation metrics
3. **Actionable insights** with counterfactuals
4. **Deep learning support** with LRP, DeepLIFT, GradCAM
5. **Ecosystem maturity** with performance, persistence, and integrations

Using **strict TDD methodology** throughout ensures quality, reliability, and maintainability as the library grows toward becoming the definitive XAI solution for Elixir.

---

*Document Version: 1.0*
*Last Updated: October 29, 2025*
*Next Review: After v0.3.0 release*
